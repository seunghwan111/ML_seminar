## 12.3 인공 신경망 훈련

#### 로지스틱 비용 함수 계산

* ```python
  def _compute_cost(self, y_enc, output):
  
      L2_term = (self.l2 * (np.sum(self.w_h ** 2) + np.sum(self.w_out ** 2)))
      print("output", output[0])
      print("log output", np.log(output[0]))
      print("log 1-output", np.log(1. - output[0]))
  
      term1 = -y_enc * (np.log(output))
      term2 = (1. - y_enc) * np.log(1. - output)
      cost = np.sum(term1 - term2) + L2_term
      return cost
  ```

  * $$
    J(w) = -\sum_{i=1}^n y^i \log(a^i) + (1-y^i)log(1-a^i)
    $$

    * $a^i$는 데이터셋 i번째 샘플의 시그모이드 활성화 출력을 의미한다.
    * 
    * np.sum(term1 - term2) 에 해당된다.

  * $$
    L2 = \lambda \lVert w \rVert_2^2 = \lambda \sum_{j=1}^m w_j^2
    $$

    * L2_term 에 해당되며, 과대적합의 정도를 줄여주기 위한 규제항이다.

  * $$
    J(w) = -\left[ \sum_{i=1}^n y^i \log(a^i) + (1-y^i)log(1-a^i) \right] + \frac{\lambda}{2} \lVert w\rVert_2^2
    $$

    * L2 규제항을 로지스틱 비용함수에 추가하면 다음 식과 같다.
    * cost = np.sum(term1 - term2) + L2_term에 해당된다.

  * 다음 식들에 대해서 네트워크에 있는 t개의 활성화 유닛 전체에 대해 규제항을 포함한 로지스틱 비용함수를 나타낸 식은 다음과 같다.

    * $$
      J(W) = -\left[ \sum_{i=1}^n\sum_{j=1}^t y_j^i \log(a_j^i) + (1-y_j^i)log(1-a_j^i) \right] + \frac{\lambda}{2} \sum_{l=1}^{L-1}\sum_{i=1}^{u_l}\sum_{j=1}^{u_{l+1}}\left( w_{j,i}^{(l)}\right)^2
      $$

    * $u_l$은 l층에 있는 유닛 개수를 나타내며, 위의 식에서 마지막 항은 패널티 항을 나타낸다.

#### 역전파 알고리즘 계산

* ```python
  def fit(self, X_train, y_train, X_valid, y_valid):
      ...
  	
      sigma_out = a_out - y_train_enc[batch_idx]
  	sigmoid_derivative_h = a_h * (1. - a_h)
  	sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)
  
  	grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)
  	grad_b_h = np.sum(sigma_h, axis=0)
  
  	grad_w_out = np.dot(a_h.T, sigma_out)
      grad_b_out = np.sum(sigma_out, axis=0)
  
      delta_w_h = (grad_w_h + self.l2*self.w_h)
      delta_b_h = grad_b_h
      self.w_h -= self.eta * delta_w_h
      self.b_h -= self.eta * delta_b_h
  
      delta_w_out = (grad_w_out + self.l2 * self.w_out)
      delta_b_out = grad_b_out
      self.w_out -= self.eta * delta_w_out
      self.b_out -= self.eta * delta_b_out
      ...
  ```

  * $$
    Z^h = A^{in}W^h\quad(은닉층의 최종 입력)\\
    A^h = \phi(Z^h)\quad(은닉층의 활성화 출력)\\
    Z^{out} = A^hW^{out}\quad(출력층의 최종 입력)\\
    A^{out} = \phi(Z^{out})\quad(출력층의 활성화 출력)
    $$

  * 역전파는 오른쪽에서 왼쪽으로 전파

    1. 출력층의 오차벡터를 계산한다.

       * sigma_out에 해당되며, $\delta^{out} = a^{out} - y$의 수식으로 정의된다.(오차벡터 = 예측클래스 벡터 - 정답 클래스 벡터)

    2. 은닉층의 오차항을 계산한다.

       * $$
         \frac{\partial\phi(z^h)}{\partial z^h} = \left( a^h \odot \left( 1-a^h\right)\right)
         $$

         sigma_derivative_h에 해당되며 sigmoid 활성화 함수의 미분식이 다음과 같이 정의된다.

       * $$
         \delta^h = \delta^{out}\left(W^{out}\right)^T\odot\frac{\partial\phi(z^h)}{\partial z^h}\\
         $$

         sigma_h에 해당되며, 은닉층의 오차항의 계산이 다음과 같이 정의된다.

       * 위의 식은 n x t 차원의 $\delta^{(out)}$과 t x h 차원의 $(W^{out})^T$의 행렬 곱셈은 n x h 차원의 $\delta^h$행렬을 만든다.

    3. 그래디언트를 계산하고 각 l층에 대한 그래디언트의 반대 방향으로 가중치를 업데이트 한다.

       * $$
         \Delta^{h} = \left( A^{in}\right)^T\delta^h\\
         \Delta^{out} = \left( A^{h}\right)^T\delta^{out}\\
         $$

         * delta_w_h, delta_b_h, delta_w_out, delta_b_out에 해당

       * $$
         \Delta^l := \Delta^l + \lambda^l W\\
         W^l := W^l - \eta\ \Delta^l
         $$

         * self.w_h, self.b_h, self.w_out, self.b_out에 대하여 계산하는 코드에 해당한다.

  * <img src="./image/fig12.png" alt="image" style="zoom:50%;" />

    

