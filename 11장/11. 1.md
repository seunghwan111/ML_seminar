## 11. 1 k-평균 알고리즘을 사용하여 유사한 객체 그룹핑



* 프로토타입 기반 군집
  * 각 클러스터가 하나의 프로토타입으로 표현된다는 뜻
  * 프로토타입은 연속적인 특성에서는 비슷한 데이터의 포인트의 센트로이드(centroid, 평균)이거나, 범주형 특성에서는 메도이드(medoid, 가장 대표되는 포인트나 가장 자주 등장하는 포인트)가 된다.
  * 대표적 알고리즘으로 k-means clustering이 있다.
  
* K-Means Algorithm
  1. 샘플 포인트에서 랜덤하게 k개의 centroid를 초기 클러스터 중심으로 선택
  2. 각 샘플을 가장 가까운 centroid $\mu^{(j)}, j \in \{1, \cdots, k\}$에  할당
  3. 할당된 샘플들의 중심으로 centroid를 이동
  4. 클러스터 할당이 변하지 않거나, 사용자가 지정한 허용오차나 최대 반복횟수에 도달할 때까지 단계2와 3을 반복
  
  
  
  * 샘플 간의 유사도 측정 : 유클라디안 거리 지표를 기반
  
    * $$
      d(x,y)^2 = \sum_{j=1}^m(x_j-y_j)^2=\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_2^2
      \\SSE = \sum_{i=1}^n\sum_{j=1}^k w^{(i, j)}\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_2^2
      $$
  
    * 클러스터 내 제곱 오차합(SSE) 또는 클러스터 관성(cluster inertia)을 반복적으로 최소화하는 방법
  
  1. <img src="./image/fig1.png" alt="image" style="zoom: 33%;" />
  2. <img src="./image/fig2.png" alt="image" style="zoom: 33%;" />
  3. <img src="./image/fig3.png" alt="image" style="zoom: 33%;" />
  4. <img src="./image/fig4.png" alt="image" style="zoom: 33%;" />
  
  * 단점 : 클러스터개수(k)를 사전에 지정해야 한다.
  
* K-Means ++ Algorithm

  * 초기 센트로이드가 서로 멀리 떨어지도록 위치시키는 것

  1. 선택한 k개의 센트로이드를 저장할 빈 집합 M을 초기화
  2. 입력 샘플에서 첫 번째 센트로이드 $\mu^{(i)}$를 랜덤하게 선택하고 M에 할당
  3. M에 있지 않은 각 샘플$x^{(i)}$에 대해 M에 있는 센트로이드까지 최소 제곱거리 $d(x^{(i)}, M)^2$을 찾음
  4. $\frac{d(\mu^{(p)}, M)^2}{\sum_id(x^{(i)}, M)^2}$과 같은 가중치가 적용된 확률 분포를 사용하여 다음 센트로이드$\mu^{(p)}$를 랜덤하게 선택
  5. k개의 센트로이드를 선택할 때가지 단계 2와 3을 반복
  6. 그다음 기본 k-means 알고리즘 수행



* 직접 군집(hard clustering)

  * 데이터셋의 샘플이 정확히 하나의 클러스터에 할당되는 알고리즘 종류
  * k-means 해당

* 간접 군집(soft clustering, fuzzy clustering이라고도 부름)

  * 샘플을 하나 이상의 클러스터에 할당한다.
  * 퍼지 C-평균(Fuzzy C-Means, FCM) 알고리즘이 대표적

* FCM

  * 포인트가 각 클러스터에 속할 확률로 표현

    * 한 샘플에 대한 클래스 소속 확률의 합은 1

  * 클러스터 소속 가중치 $w^{(i, j)}$는 0에서 1사이의 실수값을 나타내며, 퍼지계수(fuzzy coefficient) 또는 퍼지 지수(fuzzifier)라고 하는 지수 m은 1보다 크거나 같으며 퍼지의 정도를 제어한다.

    * m이 클수록 클러스터 소속확률이 작아져 더 복잡한 클러스터를 만든다.

    * 클러스터 소속 확률 계산은 다음과 같다.

    * $$
      w^{(i,j)} = \begin{bmatrix}\sum_{p=1}^k \left(\frac{\lVert x^{(i)}-\mu^{(j)}\rVert_2}{\lVert x^{(i)}-\mu^{(p)}\rVert_2}\right)^{\frac{2}{m-1}}\end{bmatrix}^{-1}
      $$

    * 3개의 클러스터 중심을 선택한다면 $x^i$가 $\mu^j$클러스터에 속할 확률은 다음과 같다.

    * $$
      w^{(i,j)} = \begin{bmatrix}\left(\frac{\lVert x^{(i)}-\mu^{(j)}\rVert_2}{\lVert x^{(i)}-\mu^{(1)}\rVert_2}\right)^{\frac{2}{m-1}} + \left(\frac{\lVert x^{(i)}-\mu^{(j)}\rVert_2}{\lVert x^{(i)}-\mu^{(2)}\rVert_2}\right)^{\frac{2}{m-1}} + \left(\frac{\lVert x^{(i)}-\mu^{(j)}\rVert_2}{\lVert x^{(i)}-\mu^{(3)}\rVert_2}\right)^{\frac{2}{m-1}}\end{bmatrix}^{-1}
      $$

    * 클러스터 중심 $\mu^{j}$는 샘플의 소속 확률을 가중치로 주어 클러스터에 속한 모든 샘플의 평균으로 계산 된다.

    * $$
      \mu^j = \frac{\sum_{i=1}^nw^{m(i,j)}x^i}{\sum_{i=1}^nw^{m(i,j)}}
      $$

  * FCM의 각 반복이 k-평균 반복보다 비용이 더 많이 드는것을 알수있지만, 전형적으로 수렴에 도달하기까지 반복 횟수가 적게 든다.
